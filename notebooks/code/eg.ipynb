{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.utils.task_group import TaskGroup\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "\n",
    "from time import strftime,localtime\n",
    "from web_scraper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ, listdir\n",
    "from db_utils import make_engine\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "db = 'nba'\n",
    "\n",
    "engine = make_engine(environ.get('USER'),environ.get('PSWD'),db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_year(year):\n",
    "        if(year > 2000):\n",
    "            return year % 2000\n",
    "        return year % 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(start_date):\n",
    "        bs = Box_scores()\n",
    "        year = int(strftime(\"%Y\",localtime()))\n",
    "\n",
    "        start_reg = 9\n",
    "        end_reg = 4\n",
    "        start_post = end_reg\n",
    "        end_post = 7\n",
    "        reg_season = True\n",
    "\n",
    "        month = int(strftime('%m',localtime()))\n",
    "        year_range = str(year-1) + \"-{:0>2d}\".format(get_last_year(year))\n",
    "\n",
    "        if month <= end_reg or month >= start_reg:\n",
    "\n",
    "                url = bs.build_url(year_range, start_date, reg_season)\n",
    "\n",
    "        elif month >= start_post and month <= end_post:\n",
    "\n",
    "                url = bs.build_url(year_range, start_date, not reg_season)\n",
    "        else:\n",
    "                return # exit dag\n",
    "\n",
    "        date = strftime('%Y-%m-%d',localtime())\n",
    "        count = 0\n",
    "        for p in bs.iter_all(url):\n",
    "                df = pd.read_html(p,flavor = 'bs4')[0]\n",
    "\n",
    "                pids, tids, gids = bs.get_player_and_team_ids(p)\n",
    "                \n",
    "                df['pids'] = pids\n",
    "                df['tids'] = tids\n",
    "                df['gids'] = gids\n",
    "                \n",
    "                fp = '../../../data/'+date+'_page_'+str(count)+'.csv'\n",
    "                df.to_csv(fp,index=False)\n",
    "\n",
    "\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_schema():\n",
    "    from pyspark.sql.types import StructType\n",
    "# path = '../../../data/'\n",
    "\n",
    "    # schema = StructType().add('Player','string').add('Team','string').add('Match Up','string').\\\n",
    "    #     add('Game Date','string').add('Season','string').add('W/L','string').add('MIN','integer').\\\n",
    "    #     add('PTS', 'integer').add('FGM', 'integer').add('FGA', 'integer').add('FG%', 'float').\\\n",
    "    #     add('3PM', 'integer').add('3PA', 'integer').add('3P%', 'float').add('FTM', 'integer').\\\n",
    "    #     add('FTA', 'integer').add('FT%', 'float').add('OREB', 'integer').add('DREB', 'integer').\\\n",
    "    #     add('REB', 'integer').add('AST', 'integer').add('STL', 'integer').add('BLK', 'integer').\\\n",
    "    #     add('TOV', 'integer').add('PF', 'integer').add('+/-', 'integer').add('FP', 'float').\\\n",
    "    #     add('pids', 'integer').add('tids', 'integer').add('gids','string')\n",
    "\n",
    "    schema = StructType().add('Player','string',False).add('Team','string',False).\\\n",
    "    add('Match Up','string',False).\\\n",
    "    add('Game Date','date',False).add('Season','string').add('W/L','string').add('MIN','integer').\\\n",
    "    add('PTS', 'integer').add('FGM', 'integer').add('FGA', 'integer').add('FG%', 'float').\\\n",
    "    add('3PM', 'integer').add('3PA', 'integer').add('3P%', 'float').add('FTM', 'integer').\\\n",
    "    add('FTA', 'integer').add('FT%', 'float').add('OREB', 'integer').add('DREB', 'integer').\\\n",
    "    add('REB', 'integer').add('AST', 'integer').add('STL', 'integer').add('BLK', 'integer').\\\n",
    "    add('TOV', 'integer').add('PF', 'integer').add('+/-', 'integer').add('FP', 'float').\\\n",
    "    add('pids', 'integer',False).add('tids', 'integer',False).add('gids','string',False)\n",
    "\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atom_schema():\n",
    "    from pyspark.sql.types import StructType\n",
    "\n",
    "    return StructType().add('id','integer',False).add('name','string',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def players(df):\n",
    "    cols = ['pids','Player']\n",
    "\n",
    "    rename_cols = ['id','name']\n",
    "\n",
    "    player_df = df[cols].select('*').distinct()\n",
    "\n",
    "    player_df = player_df.toDF(*rename_cols)\n",
    "\n",
    "    player_df.writeStream.outputMode('append').format('csv').\\\n",
    "        option('path','../../../queries/players').option('checkpointLocation','../../../checkpoints').\\\n",
    "        start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teams(df):\n",
    "    cols = ['tids','Team']\n",
    "\n",
    "    # rename_cols = ['id','name']\n",
    "\n",
    "    team_df = df[cols].select('*').distinct()\n",
    "\n",
    "    # team_df = team_df.toDF(*rename_cols)\n",
    "\n",
    "    team_df.writeStream.outputMode('append').format('csv').\\\n",
    "        option('path','../../../queries/teams').option('checkpointLocation','../../../checkpoints').\\\n",
    "        start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_scores(df):\n",
    "   cols = ['pids','tids', 'gids', 'MIN', 'PTS', 'FGM', 'FGA', '3PM', '3PA', 'FTM', 'FTA', 'OREB',\n",
    "      'DREB', 'AST', 'STL', 'BLK', 'TOV', 'PF', '+/-', 'W/L', 'Game Date', 'Match Up']\n",
    "\n",
    "   box_df = df[cols]\n",
    "\n",
    "   # rename_cols = ['player_id','team_id', 'game_id', 'mins','pts', 'fgm', 'fga', 'pm3',\n",
    "   #    'pa3', 'ftm', 'fta', 'oreb','dreb', 'ast', 'stl', 'blk', 'tov', 'pf', 'plus_minus',\n",
    "   #    'result', 'game_day', 'match_up']\n",
    "\n",
    "   # box_df = box_df.toDF(*rename_cols)\n",
    "   box_df.writeStream.outputMode('append').\\\n",
    "      format('csv').option('path','../../../queries/box_scores').\\\n",
    "      option('checkpointLocation','../../../checkpoints').start()\n",
    "\n",
    "   # box_df.createOrReplaceTempView('box_scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_score_schema():\n",
    "    from pyspark.sql.types import StructType\n",
    "\n",
    "    schema = StructType().add('player_id','integer',False).add('team_id','string',False).\\\n",
    "    add('game_id','string',False).add('mins','integer').\\\n",
    "    add('pts', 'integer').add('fgm', 'integer').add('fga', 'integer').\\\n",
    "    add('pm3', 'integer').add('pa3', 'integer').add('ftm', 'integer').\\\n",
    "    add('fta', 'integer').add('oreb', 'integer').add('dreb', 'integer').\\\n",
    "    add('ast', 'integer').add('stl', 'integer').add('blk', 'integer').\\\n",
    "    add('tov', 'integer').add('pf', 'integer').add('plus_minus', 'integer').\\\n",
    "    add('result','string').add('game_day','date').add('match_up','string')\n",
    "\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/12 18:06:55 WARN Utils: Your hostname, rpi3 resolves to a loopback address: 127.0.1.1; using 172.25.14.38 instead (on interface wlan0)\n",
      "22/06/12 18:06:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/blunt/.local/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/12 18:07:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "SparkSession.builder.config('spark.driver.extraClassPath',environ.get('CLASSPATH')).getOrCreate()\n",
    "spark = SparkSession.builder.appName('nba_player_box_score').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(spark):    \n",
    "    path = '../../../data/'\n",
    "    # stream data\n",
    "    df = spark.readStream.option('cleanSource','delete').option('sep',',').csv(path,\n",
    "    schema=extract_schema(),header=True,dateFormat='MM/dd/yyyy')\n",
    "\n",
    "    # drop unneeded columns\n",
    "    df = df.drop('Season','FP','3P%','FG%','FT%','REB')\n",
    "\n",
    "    box_scores(df)\n",
    "    players(df)\n",
    "    teams(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/12 18:10:11 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "22/06/12 18:10:12 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "22/06/12 18:10:12 WARN StreamingQueryManager: Stopping existing streaming query [id=57351088-4a62-4603-856f-dd516a3beea1, runId=37c118a7-60de-47b5-a1e6-f8db539d8042], as a new run is being started.\n",
      "22/06/12 18:10:13 WARN HadoopFSUtils: The directory file:/home/blunt/programming/data_science/data/2022-06-17_page_0.csv was not found. Was it deleted very recently?\n",
      "22/06/12 18:10:14 WARN Shell: Interrupted while joining on: Thread[Thread-19,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:327)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:333)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.$anonfun$addNewBatchByStream$2(HDFSMetadataLog.scala:173)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.addNewBatchByStream(HDFSMetadataLog.scala:171)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.add(HDFSMetadataLog.scala:116)\n",
      "\tat org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.add(CompactibleFileStreamLog.scala:168)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSourceLog.add(FileStreamSourceLog.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSource.fetchMaxOffset(FileStreamSource.scala:186)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSource.latestOffset(FileStreamSource.scala:324)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$3(MicroBatchExecution.scala:396)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:387)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:384)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:627)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:380)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:210)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\n",
      "22/06/12 18:10:14 WARN FileStreamSource$SourceFileRemover: Failed to remove file:/home/blunt/programming/data_science/data/2022-06-17_page_0.csv / skip removing file.\n",
      "22/06/12 18:10:28 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Match Up, Game Date, W/L, MIN, PTS, FGM, FGA, 3PM, 3PA, FTM, FTA, OREB, DREB, AST, STL, BLK, TOV, PF, +/-, pids, tids, gids\n",
      " Schema: Match Up, Game Date, W/L, MIN, PTS, FGM, FGA, 3PM, 3PA, FTM, FTA, OREB, DREB, AST, STL, BLK, TOV, PF, +/-, pids, tids, gids\n",
      "Expected: Match Up but found: Match Up\n",
      "CSV file: file:///home/blunt/programming/data_science/data/2022-06-12_page_3.csv\n",
      "22/06/12 18:10:28 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Match Up, Game Date, W/L, MIN, PTS, FGM, FGA, 3PM, 3PA, FTM, FTA, OREB, DREB, AST, STL, BLK, TOV, PF, +/-, pids, tids, gids\n",
      " Schema: Match Up, Game Date, W/L, MIN, PTS, FGM, FGA, 3PM, 3PA, FTM, FTA, OREB, DREB, AST, STL, BLK, TOV, PF, +/-, pids, tids, gids\n",
      "Expected: Match Up but found: Match Up\n",
      "CSV file: file:///home/blunt/programming/data_science/data/2022-06-12_page_0.csv\n",
      "22/06/12 18:10:28 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Match Up, Game Date, W/L, MIN, PTS, FGM, FGA, 3PM, 3PA, FTM, FTA, OREB, DREB, AST, STL, BLK, TOV, PF, +/-, pids, tids, gids\n",
      " Schema: Match Up, Game Date, W/L, MIN, PTS, FGM, FGA, 3PM, 3PA, FTM, FTA, OREB, DREB, AST, STL, BLK, TOV, PF, +/-, pids, tids, gids\n",
      "Expected: Match Up but found: Match Up\n",
      "CSV file: file:///home/blunt/programming/data_science/data/2022-06-12_page_2.csv\n",
      "22/06/12 18:10:28 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Match Up, Game Date, W/L, MIN, PTS, FGM, FGA, 3PM, 3PA, FTM, FTA, OREB, DREB, AST, STL, BLK, TOV, PF, +/-, pids, tids, gids\n",
      " Schema: Match Up, Game Date, W/L, MIN, PTS, FGM, FGA, 3PM, 3PA, FTM, FTA, OREB, DREB, AST, STL, BLK, TOV, PF, +/-, pids, tids, gids\n",
      "Expected: Match Up but found: Match Up\n",
      "CSV file: file:///home/blunt/programming/data_science/data/2022-06-12_page_1.csv\n",
      "22/06/12 18:10:30 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "22/06/12 18:10:30 WARN StreamingQueryManager: Stopping existing streaming query [id=57351088-4a62-4603-856f-dd516a3beea1, runId=74349924-ef52-4bed-9ef5-1a856e959f02], as a new run is being started.\n"
     ]
    }
   ],
   "source": [
    "command = 'SELECT MAX(game_day) as GD from box_scores;'\n",
    "max_date = pd.read_sql(command,engine,parse_dates=['GD'])\n",
    "max_date = max_date.loc[0,'GD']\n",
    "\n",
    "extract(max_date)\n",
    "df = transform(engine,spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new(df, table, engine):\n",
    "    ids = str(tuple(df['id']))\n",
    "    existing = pd.read_sql(f'select * from {table} where id in {ids};',engine)\n",
    "\n",
    "    merged = df.merge(existing,on='id',how='left',indicator=True)\n",
    "    merged = merged[merged['_merge']=='left_only']\n",
    "\n",
    "    added = merged.to_sql('table',engine,index=False,if_exists='append',method='multi')\n",
    "\n",
    "    print(f'{added} rows added')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def primary_keys(spark,table,engine):\n",
    "    path = f'../../../queries/{table}/*.csv'\n",
    "    df = spark.read.csv(path,\n",
    "        schema=atom_schema()).toPandas()\n",
    "\n",
    "    add_new(df,table,engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_box_scores(spark,engine):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(engine, spark):\n",
    "    path = '../../../queries/'\n",
    "\n",
    "    with TaskGroup(\"primary_keys\", tooltip=\"Add new team and player primary keys\") as primary_keys:\n",
    "        p = PythonOperator(task_id = 'Add new players',python_callable=primary_keys,\n",
    "            op_kwargs={'spark':spark,'engine':engine,'table':'players'})\n",
    "\n",
    "        t = PythonOperator(task_id = 'Add new teams',python_callable=primary_keys,\n",
    "            op_kwargs={'spark':spark,'engine':engine,'table':'teams'})\n",
    "\n",
    "        # players_df = spark.read.csv(path + 'players/*.csv',\n",
    "        # schema=atom_schema())\n",
    "    with TaskGroup(\"stats\", tooltip=\"Add new box score data\") as new_box_scores:\n",
    "        box_df = spark.read.csv(path + 'box_scores/*.csv',\n",
    "        schema=box_score_schema())\n",
    "\n",
    "    primary_keys >> new_box_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../../queries/'\n",
    "box_df = spark.readStream.option('sep',',').csv(path + 'box_scores/',\n",
    "schema=box_score_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(path + 'box_scores/*.csv',schema=box_score_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "t = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with DAG('player_box_scores_etl',default_args={'retries': 4},description='ETL DAG tutorial',\n",
    "schedule_interval='0 10 * * *',catchup=False,tags=['nba_stats']) as dag:\n",
    "\n",
    "    dag.doc_md = __doc__\n",
    "\n",
    "    SparkSession.builder.config('spark.driver.extraClassPath',environ.get('CLASSPATH')).getOrCreate()\n",
    "    spark = SparkSession.builder.appName('nba_player_box_score').getOrCreate()\n",
    "\n",
    "    command = 'SELECT MAX(game_day) as GD from box_scores;'\n",
    "    max_date = pd.read_sql(command,engine,parse_dates=['GD'])\n",
    "    max_date = max_date.loc[0,'GD']\n",
    "\n",
    "    e = PythonOperator(task_id = '',python_callable=extract,\n",
    "    op_kwargs={'start_date':max_date})\n",
    "\n",
    "    t = PythonOperator(task_id = '',python_callable=transform,\n",
    "    op_kwargs={'start_date':max_date})\n",
    "\n",
    "    l = PythonOperator(task_id = '',python_callable=load,\n",
    "    op_kwargs={'spark':spark,'engine':engine})\n",
    "\n",
    "\n",
    "    e >> t >> l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql('select * from testo').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date- datetime.timedelta(days=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkSession.builder.config('spark.driver.extraClassPath',environ.get('CLASSPATH')).getOrCreate()\n",
    "spark = SparkSession.builder.appName('nba_player_box_scores').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "path = '../../../data/'\n",
    "\n",
    "schema = StructType().add('Player','string',False).add('Team','string',False).add('Match Up','string',False).\\\n",
    "    add('Game Date','date',False).add('Season','string').add('W/L','string').add('MIN','integer').\\\n",
    "    add('PTS', 'integer').add('FGM', 'integer').add('FGA', 'integer').add('FG%', 'float').\\\n",
    "    add('3PM', 'integer').add('3PA', 'integer').add('3P%', 'float').add('FTM', 'integer').\\\n",
    "    add('FTA', 'integer').add('FT%', 'float').add('OREB', 'integer').add('DREB', 'integer').\\\n",
    "    add('REB', 'integer').add('AST', 'integer').add('STL', 'integer').add('BLK', 'integer').\\\n",
    "    add('TOV', 'integer').add('PF', 'integer').add('+/-', 'integer').add('FP', 'float').\\\n",
    "    add('pids', 'integer',False).add('tids', 'integer',False).add('gids','string',False)\n",
    "\n",
    "# df = spark.readStream.option('cleanSource','delete').text(path,wholetext=True)\n",
    "df = spark.readStream.option('cleanSource','delete').option('sep',',').csv(path,\n",
    "schema=schema,header=True,dateFormat='MM/dd/yyyy')\n",
    "\n",
    "df = df.drop('Season','FP','3P%','FG%','FT%','REB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = 'SELECT MAX(game_day) as GD from box_scores;'\n",
    "max_date = pd.read_sql(command,engine,parse_dates=['GD'])\n",
    "max_date = max_date.loc[0,'GD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream.queryName('test').outputMode('append').format('memory').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['tids','Team']\n",
    "\n",
    "teams_df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = spark.readStream.option('cleanSource','delete').option('sep',',').csv(path,inferSchema=True,\n",
    "header=True,dateFormat='MM/dd/yyyy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = teams_df.select('*').distinct()\n",
    "\n",
    "teams.writeStream.queryName('teams').outputMode('append').format('csv').\\\n",
    "        option('path','../../../queries/teams').option('checkpointLocation','../../../checkpoints').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[player_cols].writeStream.queryName('test2').outputMode('update').format('memory').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_box_scores = ['player_id','team_id', 'game_id', 'mins','pts', 'fgm', 'fga', 'pm3',\n",
    "'pa3', 'ftm', 'fta', 'oreb','dreb', 'ast', 'stl', 'blk', 'tov', 'pf', 'plus_minus',\n",
    "'result', 'game_day', 'match_up']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[box_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.toDF(*rename_box_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.writeStream.queryName('test4').outputMode('append').format('memory').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "df = df.withColumn('Game Date',to_date(df['Game Date'], 'MM/dd/yyyy'))\n",
    "# df1.printSchema()\n",
    "# df1.select(\"birth_date\").dtypes\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
