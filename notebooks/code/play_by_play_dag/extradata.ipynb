{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_convert_pbp(pageProps):\n",
    "    \n",
    "    pbp = pageProps['playByPlay']['actions']\n",
    "\n",
    "    df = pd.read_json(json.dumps(pbp),orient='records')\n",
    "    df['game_id'] = pageProps['playByPlay']['gameId']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pbp(df,filepath):\n",
    "    # filename is the gameid\n",
    "    \n",
    "    filename = df['game_id'].iloc[0]+'.csv'\n",
    "    # filepath = ''\n",
    "\n",
    "    df.to_csv(filepath+filename,index=False)\n",
    "        \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "class Play_by_play:\n",
    "    # https://www.nba.com/game/<matchup, home team>-<game_id>/box-score\n",
    "\n",
    "    def __init__(self):\n",
    "        self.root = \"https://www.nba.com/game/\" #SACvsPOR-xxxxxxxxxx/box-score\"\n",
    "        self.tail = \"/box-score\" \n",
    "\n",
    "    def build_url(self,matchup, game_id):\n",
    "\n",
    "        return self.root + matchup + '-' + game_id + self.tail\n",
    "\n",
    "\n",
    "    def get_source(self, matchup, game_id):\n",
    "        url = self.build_url(matchup,game_id)\n",
    "        response = requests.get(url)\n",
    "\n",
    "        return response.content\n",
    "    \n",
    "    def get_pages(self, df):\n",
    "        for row in df.iterrows():\n",
    "            yield self.get_source(row[1]['match_up'], row[1]['game_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "def make_engine(user, pswd, db):\n",
    "\n",
    "    return create_engine(\"mariadb+mariadbconnector://\"\\\n",
    "                        +user+\":\"\\\n",
    "                        +pswd+\"@127.0.0.1:3306/\"+db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def extract():\n",
    "    command = \"select distinct game_id, replace(replace(match_up,'.',''),' ','') \"\n",
    "    command += \"as match_up from box_scores where match_up regexp 'vs' and game_id not in \"\n",
    "    command += \"(select distinct game_id from play_by_plays) order by game_id limit 10;\"\n",
    "\n",
    "    engine = make_engine(environ.get('USER'),environ.get('PSWD'),'nba')\n",
    "\n",
    "    df = pd.read_sql(command,engine)\n",
    "\n",
    "    engine.dispose()\n",
    "\n",
    "    pbp = Play_by_play()\n",
    "\n",
    "    for page in pbp.get_pages(df):\n",
    "        save_pbp(extract_application(page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import expanduser, join as osjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType,StructField,StringType,StructType,IntegerType,FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cols(pbp):\n",
    "    \n",
    "    pbp = pbp.withColumn('action_id',pbp.actions.getItem('actionId'))\n",
    "\n",
    "    db = pbp.withColumn('player_id', pbp.actions.getItem('personId')).\\\n",
    "        withColumn('team_id', pbp.actions.getItem('teamId')).\\\n",
    "        withColumn('period', pbp.actions.getItem('period')).\\\n",
    "        withColumn('clock', pbp.actions.getItem('clock')).\\\n",
    "        withColumn('x_loc', pbp.actions.getItem('xLegacy')).\\\n",
    "        withColumn('y_loc', pbp.actions.getItem('yLegacy')).\\\n",
    "        withColumn('shot_distance', pbp.actions.getItem('shotDistance')).\\\n",
    "        withColumn('shot_result', pbp.actions.getItem('shotResult')).\\\n",
    "        withColumn('field_goal', pbp.actions.getItem('isFieldGoal')).\\\n",
    "        withColumn('home_score', pbp.actions.getItem('scoreHome').cast('integer')).\\\n",
    "        withColumn('away_score', pbp.actions.getItem('scoreAway').cast('integer')).\\\n",
    "        withColumn('total_points', pbp.actions.getItem('pointsTotal')).\\\n",
    "        withColumn('location', pbp.actions.getItem('location')).\\\n",
    "        withColumn('description', pbp.actions.getItem('description')).\\\n",
    "        withColumn('action_type', pbp.actions.getItem('actionType')).\\\n",
    "        withColumn('sub_type', pbp.actions.getItem('subType'))\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_schema():\n",
    "    schema = StructType([StructField('actions',ArrayType(\n",
    "        StructType([StructField('actionId',IntegerType()),StructField('actionNumber',IntegerType()),\n",
    "                    StructField('actionType',StringType()),StructField('clock',StringType()),\n",
    "                    StructField('description',StringType()),StructField('isFieldGoal',IntegerType()),\n",
    "                    StructField('location',StringType()),StructField('period',IntegerType()),\n",
    "                    StructField('personId',IntegerType()),StructField('playerName',StringType()),\n",
    "                    StructField('playerNameI',StringType()),StructField('pointsTotal',IntegerType()),\n",
    "                    StructField('scoreAway',StringType()),StructField('scoreHome',StringType()),\n",
    "                    StructField('shotDistance',IntegerType()),StructField('shotResult',StringType()),\n",
    "                    StructField('subType',StringType()),StructField('teamId',IntegerType()),\n",
    "                    StructField('teamTricode',StringType()),StructField('videoAvailable',IntegerType()),\n",
    "                    StructField('xLegacy',IntegerType()),StructField('yLegacy',IntegerType())]\n",
    "                )\n",
    "            )\n",
    "        ),\n",
    "    StructField('gameId',StringType(),False),\n",
    "    StructField('source',StringType() ),\n",
    "    StructField('videoAvailable',IntegerType())])\n",
    "\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform():\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import explode, regexp_extract\n",
    "\n",
    "    app_name = 'pbp'\n",
    "    directory = expanduser(osjoin('~/spark_apps',app_name))\n",
    "    input_dir = osjoin(directory,'games')\n",
    "    checkpoint_dir = osjoin(directory,'checkpoints')\n",
    "    output_dir = osjoin(directory,'to_db')\n",
    "\n",
    "    spark = SparkSession.builder.appName(app_name).getOrCreate()\n",
    "\n",
    "    df = spark.readStream.option('cleanSource','delete').schema(extract_schema()).\\\n",
    "        json(input_dir)\n",
    "    # print(df.printSchema())\n",
    "    df = df.drop('source','videoAvailable')\n",
    "\n",
    "    table_order = ['game_id', 'action_id', 'player_id', 'team_id', 'period', 'minute',\n",
    "        'seconds', 'x_loc', 'y_loc', 'shot_distance', 'shot_result',\n",
    "        'field_goal', 'home_score', 'away_score', 'total_points', 'location',\n",
    "        'description', 'action_type', 'sub_type']\n",
    "\n",
    "    pbp = df.withColumn('actions',explode(df.actions)).withColumnRenamed('gameId','game_id')\n",
    "\n",
    "    db = extract_cols(pbp)\n",
    "\n",
    "    db = db.withColumn('minute',regexp_extract(db.clock,r'PT(\\d+)M(\\d+\\.\\d+)S',1).cast('integer'))\n",
    "    db = db.withColumn('seconds',regexp_extract(db.clock,r'PT(\\d+)M(\\d+\\.\\d+)S',2).cast('float'))\n",
    "\n",
    "\n",
    "    db = db[table_order]\n",
    "    # print(db.printSchema())\n",
    "\n",
    "    db.writeStream.format(\"parquet\").option(\"path\", output_dir).\\\n",
    "        option(\"checkpointLocation\", checkpoint_dir).start()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/03 14:17:59 WARN Utils: Your hostname, rpi3 resolves to a loopback address: 127.0.1.1; using 172.25.14.38 instead (on interface wlan0)\n",
      "22/07/03 14:17:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/blunt/.local/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/07/03 14:18:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/07/03 14:18:59 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_schema():\n",
    "    schema = StructType([StructField('game_id',StringType()),StructField('action_id',IntegerType()),\n",
    "                    StructField('player_id',IntegerType()),StructField('team_id',IntegerType()),\n",
    "                    StructField('period',IntegerType()),StructField('minute',IntegerType()),\n",
    "                    StructField('seconds',FloatType()),StructField('x_loc',IntegerType()),\n",
    "                    StructField('y_loc',IntegerType()),StructField('shot_distance',IntegerType()),\n",
    "                    StructField('shot_result',StringType()),StructField('field_goal',IntegerType()),\n",
    "                    StructField('home_score',IntegerType()),StructField('away_score',IntegerType()),\n",
    "                    StructField('total_points',IntegerType()),StructField('location',StringType()),\n",
    "                    StructField('description',StringType()),StructField('action_type',StringType()),\n",
    "                    StructField('sub_type',StringType())]\n",
    "                )\n",
    "\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_db(df,epoch):\n",
    "    df.persist()\n",
    "\n",
    "    special_events = df.select('*').filter(df.team_id == 0)\n",
    "    pbp = df.select('*').filter(df.team_id != 0)\n",
    "\n",
    "    special_events.write.format('jdbc').option('url','jdbc:mysql://localhost:3306/nba').\\\n",
    "    option('user',environ.get('USER')).option('password',environ.get('PSWD')).\\\n",
    "    option(\"dbtable\", \"play_by_play_stoppages\").mode('append').save()\n",
    "\n",
    "\n",
    "    pbp.write.format('jdbc').option('url','jdbc:mysql://localhost:3306/nba').\\\n",
    "    option('user',environ.get('USER')).option('password',environ.get('PSWD')).\\\n",
    "    option(\"dbtable\", \"play_by_plays\").mode('append').save()\n",
    "\n",
    "    \n",
    "    df.unpersist()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load():\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    SparkSession.builder.config('spark.driver.extraClassPath',environ.get('SPARK_JDBC')).getOrCreate()\n",
    "    spark = SparkSession.builder.appName('pbp').getOrCreate()\n",
    "\n",
    "    df = spark.readStream.schema(load_schema()).parquet('to_db',mergeSchema=True)\n",
    "\n",
    "    df.writeStream.outputMode('append').foreachBatch(write_to_db).start()   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/03 14:59:26 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-3d93207a-ed2e-44bb-8cbb-8c5469ed1ecb. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/07/03 14:59:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PBP schema:\n",
    "game_id := char(10)\n",
    "\n",
    "clock := --> minute := smallint, seconds := float \n",
    "\n",
    "period := smallint unsigned\n",
    "\n",
    "teamId := int FK references teams\n",
    "\n",
    "personId := int FK references players\n",
    "\n",
    "xLegacy := int\n",
    "\n",
    "yLegacy := int\n",
    "\n",
    "shotDistance := int\n",
    "\n",
    "shotResult := text\n",
    "\n",
    "isFieldGoal := smallint\n",
    "\n",
    "scoreHome := int \n",
    "\n",
    "scoreAway := int \n",
    "\n",
    "pointsTotal := int \n",
    "\n",
    "location := char\n",
    "\n",
    "description := text\n",
    "\n",
    "actionType := text\n",
    "\n",
    "subType := text\n",
    "\n",
    "actionId := int unsigned NOT NULL (indexes events within game's play by play)\n",
    "\n",
    "#### create two tabels: one with player data the other with \"special events\" (where teamId == 0). Can join the two by union/df stacking in application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_application(html):\n",
    "        \n",
    "    soup = BeautifulSoup(html)\n",
    "    app_script = soup.find('script',{'type':\"application/json\"})\n",
    "\n",
    "    return json.loads(app_script.decode_contents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pbp(data):\n",
    "    # filename is the gameid\n",
    "    \n",
    "    data = data['props']['pageProps']['playByPlay']\n",
    "\n",
    "    filename = 'games/'+data['gameId']+'.json'\n",
    "\n",
    "    with open(filename,'w') as f:\n",
    "        json.dump(data,f)\n",
    "        \n",
    "    return "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
